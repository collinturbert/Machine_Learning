{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7754c152-bc52-44a5-b89d-f17711202ab7",
   "metadata": {},
   "source": [
    "## Question 2.1\n",
    "> Desribe a situation or problem from your job, everyday life, current events, etc., for which a classification model would be appropriate. List some (up to 5) predictors that you might use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7471a5ca-739c-422d-ba70-ee99a8402f50",
   "metadata": {},
   "source": [
    "In K-12 education educators are responsible for helping to identify students that potentially have a learning disability. Some potential predictors could be:\n",
    "1. Verbal language ability (could use various language tests already given depending on age)\n",
    "2. Math ability (using standardized tests)\n",
    "3. Reading/Writing ability (using standardized tests)\n",
    "4. Academic performance (grades disaggregated by subject and viewed over time)\n",
    "5. Attendance\n",
    "6. Parents highest level of education\n",
    "7. Known medical issues\n",
    "8. Academic background (access to education in the past)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69cf67f-23d1-4765-94ad-66781940fb6a",
   "metadata": {},
   "source": [
    "## Question 2.2\n",
    "> The files credit_card_data.txt (without headers) and credit_card_data-headers.txt (with headers) contain a dataset with 654 data points, 6 continuous and 4 binary predictor variables.  It has anonymized credit card applications with a binary response variable (last column) indicating if the application was positive or negative. The dataset is the “Credit Approval Data Set” from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Credit+Approval) without the categorical variables and without data points that have missing values.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f38658-f5f5-47e0-91e6-cb89e69c8cc9",
   "metadata": {},
   "source": [
    "#### Install Libraries\n",
    "This question needs the kernlab and kknn libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9128f7f0-5ef1-4be5-8348-b402658d96b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(kernlab)\n",
    "library(kknn)\n",
    "library(vtable)\n",
    "library(htmltools)\n",
    "suppressWarnings(library(ggplot2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9801f54-bf18-4748-9701-bc211b5403d4",
   "metadata": {},
   "source": [
    "#### Load Data\n",
    "The data set was provided in the homework .zip folder and I am choosing to use the data that includes headers so they are available to reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2a4504c-b1f3-4b0a-9308-8a10a0b741f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data <- read.table(\"credit_card_data-headers.txt\", header = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e8b88e-09ea-4172-bf53-3c54c31ebb25",
   "metadata": {},
   "source": [
    "#### Examine Data\n",
    "Now that the data is loaded I want to examine it to get a better idea of what it looks like. I'm interested in actually seeing the data but also infomration about the data such as data type and ranges of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e642977-31a5-4244-955b-25b0b7b8c265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>A1</th><th scope=col>A2</th><th scope=col>A3</th><th scope=col>A8</th><th scope=col>A9</th><th scope=col>A10</th><th scope=col>A11</th><th scope=col>A12</th><th scope=col>A14</th><th scope=col>A15</th><th scope=col>R1</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>integer</td><td>numeric</td><td>numeric</td><td>numeric</td><td>integer</td><td>integer</td><td>integer</td><td>integer</td><td>integer</td><td>integer</td><td>integer</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{lllllllllll}\n",
       " A1 & A2 & A3 & A8 & A9 & A10 & A11 & A12 & A14 & A15 & R1\\\\\n",
       "\\hline\n",
       "\t integer & numeric & numeric & numeric & integer & integer & integer & integer & integer & integer & integer\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| A1 | A2 | A3 | A8 | A9 | A10 | A11 | A12 | A14 | A15 | R1 |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| integer | numeric | numeric | numeric | integer | integer | integer | integer | integer | integer | integer |\n",
       "\n"
      ],
      "text/plain": [
       "     A1      A2      A3      A8      A9      A10     A11     A12     A14    \n",
       "[1,] integer numeric numeric numeric integer integer integer integer integer\n",
       "     A15     R1     \n",
       "[1,] integer integer"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Output the data class for each column transposed\n",
    "t(sapply(data, class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "999d6c35-1ec5-4e1f-a91e-ef1da9e923ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>A1</th><th scope=col>A2</th><th scope=col>A3</th><th scope=col>A8</th><th scope=col>A9</th><th scope=col>A10</th><th scope=col>A11</th><th scope=col>A12</th><th scope=col>A14</th><th scope=col>A15</th><th scope=col>R1</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1    </td><td>30.83</td><td>0.000</td><td>1.25 </td><td>1    </td><td>0    </td><td>1    </td><td>1    </td><td>202  </td><td>  0  </td><td>1    </td></tr>\n",
       "\t<tr><td>0    </td><td>58.67</td><td>4.460</td><td>3.04 </td><td>1    </td><td>0    </td><td>6    </td><td>1    </td><td> 43  </td><td>560  </td><td>1    </td></tr>\n",
       "\t<tr><td>0    </td><td>24.50</td><td>0.500</td><td>1.50 </td><td>1    </td><td>1    </td><td>0    </td><td>1    </td><td>280  </td><td>824  </td><td>1    </td></tr>\n",
       "\t<tr><td>1    </td><td>27.83</td><td>1.540</td><td>3.75 </td><td>1    </td><td>0    </td><td>5    </td><td>0    </td><td>100  </td><td>  3  </td><td>1    </td></tr>\n",
       "\t<tr><td>1    </td><td>20.17</td><td>5.625</td><td>1.71 </td><td>1    </td><td>1    </td><td>0    </td><td>1    </td><td>120  </td><td>  0  </td><td>1    </td></tr>\n",
       "\t<tr><td>1    </td><td>32.08</td><td>4.000</td><td>2.50 </td><td>1    </td><td>1    </td><td>0    </td><td>0    </td><td>360  </td><td>  0  </td><td>1    </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllllll}\n",
       " A1 & A2 & A3 & A8 & A9 & A10 & A11 & A12 & A14 & A15 & R1\\\\\n",
       "\\hline\n",
       "\t 1     & 30.83 & 0.000 & 1.25  & 1     & 0     & 1     & 1     & 202   &   0   & 1    \\\\\n",
       "\t 0     & 58.67 & 4.460 & 3.04  & 1     & 0     & 6     & 1     &  43   & 560   & 1    \\\\\n",
       "\t 0     & 24.50 & 0.500 & 1.50  & 1     & 1     & 0     & 1     & 280   & 824   & 1    \\\\\n",
       "\t 1     & 27.83 & 1.540 & 3.75  & 1     & 0     & 5     & 0     & 100   &   3   & 1    \\\\\n",
       "\t 1     & 20.17 & 5.625 & 1.71  & 1     & 1     & 0     & 1     & 120   &   0   & 1    \\\\\n",
       "\t 1     & 32.08 & 4.000 & 2.50  & 1     & 1     & 0     & 0     & 360   &   0   & 1    \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| A1 | A2 | A3 | A8 | A9 | A10 | A11 | A12 | A14 | A15 | R1 |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1     | 30.83 | 0.000 | 1.25  | 1     | 0     | 1     | 1     | 202   |   0   | 1     |\n",
       "| 0     | 58.67 | 4.460 | 3.04  | 1     | 0     | 6     | 1     |  43   | 560   | 1     |\n",
       "| 0     | 24.50 | 0.500 | 1.50  | 1     | 1     | 0     | 1     | 280   | 824   | 1     |\n",
       "| 1     | 27.83 | 1.540 | 3.75  | 1     | 0     | 5     | 0     | 100   |   3   | 1     |\n",
       "| 1     | 20.17 | 5.625 | 1.71  | 1     | 1     | 0     | 1     | 120   |   0   | 1     |\n",
       "| 1     | 32.08 | 4.000 | 2.50  | 1     | 1     | 0     | 0     | 360   |   0   | 1     |\n",
       "\n"
      ],
      "text/plain": [
       "  A1 A2    A3    A8   A9 A10 A11 A12 A14 A15 R1\n",
       "1 1  30.83 0.000 1.25 1  0   1   1   202   0 1 \n",
       "2 0  58.67 4.460 3.04 1  0   6   1    43 560 1 \n",
       "3 0  24.50 0.500 1.50 1  1   0   1   280 824 1 \n",
       "4 1  27.83 1.540 3.75 1  0   5   0   100   3 1 \n",
       "5 1  20.17 5.625 1.71 1  1   0   1   120   0 1 \n",
       "6 1  32.08 4.000 2.50 1  1   0   0   360   0 1 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>A1</th><th scope=col>A2</th><th scope=col>A3</th><th scope=col>A8</th><th scope=col>A9</th><th scope=col>A10</th><th scope=col>A11</th><th scope=col>A12</th><th scope=col>A14</th><th scope=col>A15</th><th scope=col>R1</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>649</th><td>1     </td><td>40.58 </td><td> 3.290</td><td>3.50  </td><td>0     </td><td>1     </td><td>0     </td><td>0     </td><td>400   </td><td>  0   </td><td>0     </td></tr>\n",
       "\t<tr><th scope=row>650</th><td>1     </td><td>21.08 </td><td>10.085</td><td>1.25  </td><td>0     </td><td>1     </td><td>0     </td><td>1     </td><td>260   </td><td>  0   </td><td>0     </td></tr>\n",
       "\t<tr><th scope=row>651</th><td>0     </td><td>22.67 </td><td> 0.750</td><td>2.00  </td><td>0     </td><td>0     </td><td>2     </td><td>0     </td><td>200   </td><td>394   </td><td>0     </td></tr>\n",
       "\t<tr><th scope=row>652</th><td>0     </td><td>25.25 </td><td>13.500</td><td>2.00  </td><td>0     </td><td>0     </td><td>1     </td><td>0     </td><td>200   </td><td>  1   </td><td>0     </td></tr>\n",
       "\t<tr><th scope=row>653</th><td>1     </td><td>17.92 </td><td> 0.205</td><td>0.04  </td><td>0     </td><td>1     </td><td>0     </td><td>1     </td><td>280   </td><td>750   </td><td>0     </td></tr>\n",
       "\t<tr><th scope=row>654</th><td>1     </td><td>35.00 </td><td> 3.375</td><td>8.29  </td><td>0     </td><td>1     </td><td>0     </td><td>0     </td><td>  0   </td><td>  0   </td><td>0     </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllllll}\n",
       "  & A1 & A2 & A3 & A8 & A9 & A10 & A11 & A12 & A14 & A15 & R1\\\\\n",
       "\\hline\n",
       "\t649 & 1      & 40.58  &  3.290 & 3.50   & 0      & 1      & 0      & 0      & 400    &   0    & 0     \\\\\n",
       "\t650 & 1      & 21.08  & 10.085 & 1.25   & 0      & 1      & 0      & 1      & 260    &   0    & 0     \\\\\n",
       "\t651 & 0      & 22.67  &  0.750 & 2.00   & 0      & 0      & 2      & 0      & 200    & 394    & 0     \\\\\n",
       "\t652 & 0      & 25.25  & 13.500 & 2.00   & 0      & 0      & 1      & 0      & 200    &   1    & 0     \\\\\n",
       "\t653 & 1      & 17.92  &  0.205 & 0.04   & 0      & 1      & 0      & 1      & 280    & 750    & 0     \\\\\n",
       "\t654 & 1      & 35.00  &  3.375 & 8.29   & 0      & 1      & 0      & 0      &   0    &   0    & 0     \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | A1 | A2 | A3 | A8 | A9 | A10 | A11 | A12 | A14 | A15 | R1 |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 649 | 1      | 40.58  |  3.290 | 3.50   | 0      | 1      | 0      | 0      | 400    |   0    | 0      |\n",
       "| 650 | 1      | 21.08  | 10.085 | 1.25   | 0      | 1      | 0      | 1      | 260    |   0    | 0      |\n",
       "| 651 | 0      | 22.67  |  0.750 | 2.00   | 0      | 0      | 2      | 0      | 200    | 394    | 0      |\n",
       "| 652 | 0      | 25.25  | 13.500 | 2.00   | 0      | 0      | 1      | 0      | 200    |   1    | 0      |\n",
       "| 653 | 1      | 17.92  |  0.205 | 0.04   | 0      | 1      | 0      | 1      | 280    | 750    | 0      |\n",
       "| 654 | 1      | 35.00  |  3.375 | 8.29   | 0      | 1      | 0      | 0      |   0    |   0    | 0      |\n",
       "\n"
      ],
      "text/plain": [
       "    A1 A2    A3     A8   A9 A10 A11 A12 A14 A15 R1\n",
       "649 1  40.58  3.290 3.50 0  1   0   0   400   0 0 \n",
       "650 1  21.08 10.085 1.25 0  1   0   1   260   0 0 \n",
       "651 0  22.67  0.750 2.00 0  0   2   0   200 394 0 \n",
       "652 0  25.25 13.500 2.00 0  0   1   0   200   1 0 \n",
       "653 1  17.92  0.205 0.04 0  1   0   1   280 750 0 \n",
       "654 1  35.00  3.375 8.29 0  1   0   0     0   0 0 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Look at the first and last few rows of the data\n",
    "head(data)\n",
    "tail(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68d6ce3a-f0e2-4ab5-88d0-e7785daf92c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>A1</th><td> 0.00    </td><td>     1.00</td></tr>\n",
       "\t<tr><th scope=row>A2</th><td>13.75    </td><td>    80.25</td></tr>\n",
       "\t<tr><th scope=row>A3</th><td> 0.00    </td><td>    28.00</td></tr>\n",
       "\t<tr><th scope=row>A8</th><td> 0.00    </td><td>    28.50</td></tr>\n",
       "\t<tr><th scope=row>A9</th><td> 0.00    </td><td>     1.00</td></tr>\n",
       "\t<tr><th scope=row>A10</th><td> 0.00    </td><td>     1.00</td></tr>\n",
       "\t<tr><th scope=row>A11</th><td> 0.00    </td><td>    67.00</td></tr>\n",
       "\t<tr><th scope=row>A12</th><td> 0.00    </td><td>     1.00</td></tr>\n",
       "\t<tr><th scope=row>A14</th><td> 0.00    </td><td>  2000.00</td></tr>\n",
       "\t<tr><th scope=row>A15</th><td> 0.00    </td><td>100000.00</td></tr>\n",
       "\t<tr><th scope=row>R1</th><td> 0.00    </td><td>     1.00</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "\tA1 &  0.00     &      1.00\\\\\n",
       "\tA2 & 13.75     &     80.25\\\\\n",
       "\tA3 &  0.00     &     28.00\\\\\n",
       "\tA8 &  0.00     &     28.50\\\\\n",
       "\tA9 &  0.00     &      1.00\\\\\n",
       "\tA10 &  0.00     &      1.00\\\\\n",
       "\tA11 &  0.00     &     67.00\\\\\n",
       "\tA12 &  0.00     &      1.00\\\\\n",
       "\tA14 &  0.00     &   2000.00\\\\\n",
       "\tA15 &  0.00     & 100000.00\\\\\n",
       "\tR1 &  0.00     &      1.00\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| A1 |  0.00     |      1.00 |\n",
       "| A2 | 13.75     |     80.25 |\n",
       "| A3 |  0.00     |     28.00 |\n",
       "| A8 |  0.00     |     28.50 |\n",
       "| A9 |  0.00     |      1.00 |\n",
       "| A10 |  0.00     |      1.00 |\n",
       "| A11 |  0.00     |     67.00 |\n",
       "| A12 |  0.00     |      1.00 |\n",
       "| A14 |  0.00     |   2000.00 |\n",
       "| A15 |  0.00     | 100000.00 |\n",
       "| R1 |  0.00     |      1.00 |\n",
       "\n"
      ],
      "text/plain": [
       "    [,1]  [,2]     \n",
       "A1   0.00      1.00\n",
       "A2  13.75     80.25\n",
       "A3   0.00     28.00\n",
       "A8   0.00     28.50\n",
       "A9   0.00      1.00\n",
       "A10  0.00      1.00\n",
       "A11  0.00     67.00\n",
       "A12  0.00      1.00\n",
       "A14  0.00   2000.00\n",
       "A15  0.00 100000.00\n",
       "R1   0.00      1.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Output the range of all the values for each column as a table\n",
    "t(sapply(data, range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cf07293-9633-4310-8f5d-d967598cda54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1</li>\n",
       "\t<li>0</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1\n",
       "\\item 0\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1\n",
       "2. 0\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 1 0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check that the R1 column only contains 1 and 2 and output a summary table (doesn't show in notebook)\n",
    "unique(data$R1)\n",
    "sumtable(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00d2f466-0ff1-4f43-9300-21b2be512f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows   Columns"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "\t<tr><td>654</td><td>11 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{ll}\n",
       "\t 654 & 11 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| 654 | 11  |\n",
       "\n"
      ],
      "text/plain": [
       "     [,1] [,2]\n",
       "[1,] 654  11  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Double check size of the data\n",
    "cat('Rows',' ','Columns')\n",
    "t(dim(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b3b819-e97b-4af8-b667-b0b1851583c5",
   "metadata": {},
   "source": [
    "This lets me know that I have a table with 654 rows and 11 columns (10 attributes and one result). The data is a combination of integer and float (numeric) values with A1, A9, A10, A12, and R1 being categorical (1 or 0). Looking at the UC Irvine website linked above, they list information about the dataset and I can see that our dataset has been cleaned. Missing values have been removed, categorical variables with more than 2 categories have been dropped, and categories have been turned into numerical values (1 or 0). It's also important to note the wide range in values that some of our attributes possess, meaning scaling will be important. I also know that this dataset is being using to determine credit approval for credit card applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61692ab-64dc-450a-8aae-7870551eccd0",
   "metadata": {},
   "source": [
    "### SVM Classification\n",
    "Next I am going to fit the data with a SVM model and test to see how well it predicts credit approval. I want to look a range of C (NOT lambda but has similar effect in our model) values and determine which value will be a best fit for the model. It's important to note that the higher the lambda the more weight we put on creating a larger margin and the less weight we put on minimizing errors but that C is the inverse of that. I'm also interested in trying different kernals to see how they change the results which is described as optional question #2 so will include that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54116fd9-68af-46c4-99be-f6fb81b055da",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_values <- c(.000000001, .00000001, .0000001, .000001, .00001, .0001, .001, .01, .1, 1, 10, 100, 1000, 10000, 100000, 1000000, 10000000, 100000000)\n",
    "kernal_values <- c(\"vanilladot\", \"rbfdot\", \"polydot\", \"tanhdot\", \"laplacedot\", \"besseldot\", \"anovadot\", \"splinedot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0196fd-8421-4ae6-9eba-b472c34ad663",
   "metadata": {},
   "source": [
    "I want to save the results of the various C values and kernals to a dataframe that I can call after testing all the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fcd8395-628d-411d-b04e-41381b99f17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df <- data.frame('C_Value' = numeric(), 'Kernal_Name' = character(), 'Accuracy' = numeric(), stringsAsFactors = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb304f8-5c03-4156-84dc-12a1092a41a3",
   "metadata": {},
   "source": [
    "Next I will loop through the various C values and kernals training our model on the data with those inputs. Then I will test the accuracy of the model in predicting credit approval for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbf93ec2-1f36-422f-ba09-d0c9c6e42c06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n",
      " Setting default kernel parameters  \n"
     ]
    }
   ],
   "source": [
    "for (kern_val in kernal_values) {\n",
    "    for (C_val in C_values) {\n",
    "    \tvsm_model <- ksvm(as.matrix(data[,1:10]),as.factor(data[,11]),type=\"C-svc\",kernel=kern_val,C=C_val,scaled=TRUE)\n",
    "    \tpred <- predict(vsm_model,data[,1:10])\n",
    "    \taccuracy <- round((sum(pred == data[,11]) / nrow(data))*100,4)\n",
    "        results_df <- rbind(results_df, data.frame(C_Value = C_val, Kernal_Name = kern_val, Accuracy = accuracy))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5771f8ef-20a9-42dd-85f6-5b951d349b36",
   "metadata": {},
   "source": [
    "The entire results data frame is printed below so I can get an idea of how our various C values and kernals did at predicting credit card approval for our given dataset. I did not include the entire output to keep the homework submission from becoming to lengthy. I will use this infromation to answer questsion 2.2 - Part 1 and 2.2 - Part 2 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b25575b-a134-457f-94ec-61a16df018ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>C_Value</th><th scope=col>Kernal_Name</th><th scope=col>Accuracy</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1e-09     </td><td>vanilladot</td><td>54.7401   </td></tr>\n",
       "\t<tr><td>1e-08     </td><td>vanilladot</td><td>54.7401   </td></tr>\n",
       "\t<tr><td>1e-07     </td><td>vanilladot</td><td>54.7401   </td></tr>\n",
       "\t<tr><td>1e-06     </td><td>vanilladot</td><td>54.7401   </td></tr>\n",
       "\t<tr><td>1e-05     </td><td>vanilladot</td><td>54.7401   </td></tr>\n",
       "\t<tr><td>1e-04     </td><td>vanilladot</td><td>54.7401   </td></tr>\n",
       "\t<tr><td>1e-03     </td><td>vanilladot</td><td>83.7920   </td></tr>\n",
       "\t<tr><td>1e-02     </td><td>vanilladot</td><td>86.3914   </td></tr>\n",
       "\t<tr><td>1e-01     </td><td>vanilladot</td><td>86.3914   </td></tr>\n",
       "\t<tr><td>1e+00     </td><td>vanilladot</td><td>86.3914   </td></tr>\n",
       "\t<tr><td>1e+01     </td><td>vanilladot</td><td>86.3914   </td></tr>\n",
       "\t<tr><td>1e+02     </td><td>vanilladot</td><td>86.3914   </td></tr>\n",
       "\t<tr><td>1e+03     </td><td>vanilladot</td><td>86.2385   </td></tr>\n",
       "\t<tr><td>1e+04     </td><td>vanilladot</td><td>86.2385   </td></tr>\n",
       "\t<tr><td>1e+05     </td><td>vanilladot</td><td>86.3914   </td></tr>\n",
       "\t<tr><td>1e+06     </td><td>vanilladot</td><td>62.5382   </td></tr>\n",
       "\t<tr><td>1e+07     </td><td>vanilladot</td><td>54.5872   </td></tr>\n",
       "\t<tr><td>1e+08     </td><td>vanilladot</td><td>66.3609   </td></tr>\n",
       "\t<tr><td>1e-09     </td><td>rbfdot    </td><td>54.7401   </td></tr>\n",
       "\t<tr><td>1e-08     </td><td>rbfdot    </td><td>54.7401   </td></tr>\n",
       "\t<tr><td>1e-07     </td><td>rbfdot    </td><td>54.7401   </td></tr>\n",
       "\t<tr><td>1e-06     </td><td>rbfdot    </td><td>54.7401   </td></tr>\n",
       "\t<tr><td>1e-05     </td><td>rbfdot    </td><td>54.7401   </td></tr>\n",
       "\t<tr><td>1e-04     </td><td>rbfdot    </td><td>54.7401   </td></tr>\n",
       "\t<tr><td>1e-03     </td><td>rbfdot    </td><td>54.7401   </td></tr>\n",
       "\t<tr><td>1e-02     </td><td>rbfdot    </td><td>56.1162   </td></tr>\n",
       "\t<tr><td>1e-01     </td><td>rbfdot    </td><td>85.9327   </td></tr>\n",
       "\t<tr><td>1e+00     </td><td>rbfdot    </td><td>87.1560   </td></tr>\n",
       "\t<tr><td>1e+01     </td><td>rbfdot    </td><td>91.4373   </td></tr>\n",
       "\t<tr><td>1e+02     </td><td>rbfdot    </td><td>95.7187   </td></tr>\n",
       "\t<tr><td>...</td><td>...</td><td>...</td></tr>\n",
       "\t<tr><td>1e-03    </td><td>anovadot </td><td>58.8685  </td></tr>\n",
       "\t<tr><td>1e-02    </td><td>anovadot </td><td>86.2385  </td></tr>\n",
       "\t<tr><td>1e-01    </td><td>anovadot </td><td>86.2385  </td></tr>\n",
       "\t<tr><td>1e+00    </td><td>anovadot </td><td>86.3914  </td></tr>\n",
       "\t<tr><td>1e+01    </td><td>anovadot </td><td>87.3089  </td></tr>\n",
       "\t<tr><td>1e+02    </td><td>anovadot </td><td>90.6728  </td></tr>\n",
       "\t<tr><td>1e+03    </td><td>anovadot </td><td>90.6728  </td></tr>\n",
       "\t<tr><td>1e+04    </td><td>anovadot </td><td>90.8257  </td></tr>\n",
       "\t<tr><td>1e+05    </td><td>anovadot </td><td>88.5321  </td></tr>\n",
       "\t<tr><td>1e+06    </td><td>anovadot </td><td>81.9572  </td></tr>\n",
       "\t<tr><td>1e+07    </td><td>anovadot </td><td>68.9602  </td></tr>\n",
       "\t<tr><td>1e+08    </td><td>anovadot </td><td>77.3700  </td></tr>\n",
       "\t<tr><td>1e-09    </td><td>splinedot</td><td>54.7401  </td></tr>\n",
       "\t<tr><td>1e-08    </td><td>splinedot</td><td>54.8930  </td></tr>\n",
       "\t<tr><td>1e-07    </td><td>splinedot</td><td>54.8930  </td></tr>\n",
       "\t<tr><td>1e-06    </td><td>splinedot</td><td>55.8104  </td></tr>\n",
       "\t<tr><td>1e-05    </td><td>splinedot</td><td>57.7982  </td></tr>\n",
       "\t<tr><td>1e-04    </td><td>splinedot</td><td>62.3853  </td></tr>\n",
       "\t<tr><td>1e-03    </td><td>splinedot</td><td>78.2875  </td></tr>\n",
       "\t<tr><td>1e-02    </td><td>splinedot</td><td>81.0398  </td></tr>\n",
       "\t<tr><td>1e-01    </td><td>splinedot</td><td>94.4954  </td></tr>\n",
       "\t<tr><td>1e+00    </td><td>splinedot</td><td>96.6361  </td></tr>\n",
       "\t<tr><td>1e+01    </td><td>splinedot</td><td>97.8593  </td></tr>\n",
       "\t<tr><td>1e+02    </td><td>splinedot</td><td>97.8593  </td></tr>\n",
       "\t<tr><td>1e+03    </td><td>splinedot</td><td>97.8593  </td></tr>\n",
       "\t<tr><td>1e+04    </td><td>splinedot</td><td>97.8593  </td></tr>\n",
       "\t<tr><td>1e+05    </td><td>splinedot</td><td>97.8593  </td></tr>\n",
       "\t<tr><td>1e+06    </td><td>splinedot</td><td>94.3425  </td></tr>\n",
       "\t<tr><td>1e+07    </td><td>splinedot</td><td>87.7676  </td></tr>\n",
       "\t<tr><td>1e+08    </td><td>splinedot</td><td>86.5443  </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " C\\_Value & Kernal\\_Name & Accuracy\\\\\n",
       "\\hline\n",
       "\t 1e-09      & vanilladot & 54.7401   \\\\\n",
       "\t 1e-08      & vanilladot & 54.7401   \\\\\n",
       "\t 1e-07      & vanilladot & 54.7401   \\\\\n",
       "\t 1e-06      & vanilladot & 54.7401   \\\\\n",
       "\t 1e-05      & vanilladot & 54.7401   \\\\\n",
       "\t 1e-04      & vanilladot & 54.7401   \\\\\n",
       "\t 1e-03      & vanilladot & 83.7920   \\\\\n",
       "\t 1e-02      & vanilladot & 86.3914   \\\\\n",
       "\t 1e-01      & vanilladot & 86.3914   \\\\\n",
       "\t 1e+00      & vanilladot & 86.3914   \\\\\n",
       "\t 1e+01      & vanilladot & 86.3914   \\\\\n",
       "\t 1e+02      & vanilladot & 86.3914   \\\\\n",
       "\t 1e+03      & vanilladot & 86.2385   \\\\\n",
       "\t 1e+04      & vanilladot & 86.2385   \\\\\n",
       "\t 1e+05      & vanilladot & 86.3914   \\\\\n",
       "\t 1e+06      & vanilladot & 62.5382   \\\\\n",
       "\t 1e+07      & vanilladot & 54.5872   \\\\\n",
       "\t 1e+08      & vanilladot & 66.3609   \\\\\n",
       "\t 1e-09      & rbfdot     & 54.7401   \\\\\n",
       "\t 1e-08      & rbfdot     & 54.7401   \\\\\n",
       "\t 1e-07      & rbfdot     & 54.7401   \\\\\n",
       "\t 1e-06      & rbfdot     & 54.7401   \\\\\n",
       "\t 1e-05      & rbfdot     & 54.7401   \\\\\n",
       "\t 1e-04      & rbfdot     & 54.7401   \\\\\n",
       "\t 1e-03      & rbfdot     & 54.7401   \\\\\n",
       "\t 1e-02      & rbfdot     & 56.1162   \\\\\n",
       "\t 1e-01      & rbfdot     & 85.9327   \\\\\n",
       "\t 1e+00      & rbfdot     & 87.1560   \\\\\n",
       "\t 1e+01      & rbfdot     & 91.4373   \\\\\n",
       "\t 1e+02      & rbfdot     & 95.7187   \\\\\n",
       "\t ... & ... & ...\\\\\n",
       "\t 1e-03     & anovadot  & 58.8685  \\\\\n",
       "\t 1e-02     & anovadot  & 86.2385  \\\\\n",
       "\t 1e-01     & anovadot  & 86.2385  \\\\\n",
       "\t 1e+00     & anovadot  & 86.3914  \\\\\n",
       "\t 1e+01     & anovadot  & 87.3089  \\\\\n",
       "\t 1e+02     & anovadot  & 90.6728  \\\\\n",
       "\t 1e+03     & anovadot  & 90.6728  \\\\\n",
       "\t 1e+04     & anovadot  & 90.8257  \\\\\n",
       "\t 1e+05     & anovadot  & 88.5321  \\\\\n",
       "\t 1e+06     & anovadot  & 81.9572  \\\\\n",
       "\t 1e+07     & anovadot  & 68.9602  \\\\\n",
       "\t 1e+08     & anovadot  & 77.3700  \\\\\n",
       "\t 1e-09     & splinedot & 54.7401  \\\\\n",
       "\t 1e-08     & splinedot & 54.8930  \\\\\n",
       "\t 1e-07     & splinedot & 54.8930  \\\\\n",
       "\t 1e-06     & splinedot & 55.8104  \\\\\n",
       "\t 1e-05     & splinedot & 57.7982  \\\\\n",
       "\t 1e-04     & splinedot & 62.3853  \\\\\n",
       "\t 1e-03     & splinedot & 78.2875  \\\\\n",
       "\t 1e-02     & splinedot & 81.0398  \\\\\n",
       "\t 1e-01     & splinedot & 94.4954  \\\\\n",
       "\t 1e+00     & splinedot & 96.6361  \\\\\n",
       "\t 1e+01     & splinedot & 97.8593  \\\\\n",
       "\t 1e+02     & splinedot & 97.8593  \\\\\n",
       "\t 1e+03     & splinedot & 97.8593  \\\\\n",
       "\t 1e+04     & splinedot & 97.8593  \\\\\n",
       "\t 1e+05     & splinedot & 97.8593  \\\\\n",
       "\t 1e+06     & splinedot & 94.3425  \\\\\n",
       "\t 1e+07     & splinedot & 87.7676  \\\\\n",
       "\t 1e+08     & splinedot & 86.5443  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| C_Value | Kernal_Name | Accuracy |\n",
       "|---|---|---|\n",
       "| 1e-09      | vanilladot | 54.7401    |\n",
       "| 1e-08      | vanilladot | 54.7401    |\n",
       "| 1e-07      | vanilladot | 54.7401    |\n",
       "| 1e-06      | vanilladot | 54.7401    |\n",
       "| 1e-05      | vanilladot | 54.7401    |\n",
       "| 1e-04      | vanilladot | 54.7401    |\n",
       "| 1e-03      | vanilladot | 83.7920    |\n",
       "| 1e-02      | vanilladot | 86.3914    |\n",
       "| 1e-01      | vanilladot | 86.3914    |\n",
       "| 1e+00      | vanilladot | 86.3914    |\n",
       "| 1e+01      | vanilladot | 86.3914    |\n",
       "| 1e+02      | vanilladot | 86.3914    |\n",
       "| 1e+03      | vanilladot | 86.2385    |\n",
       "| 1e+04      | vanilladot | 86.2385    |\n",
       "| 1e+05      | vanilladot | 86.3914    |\n",
       "| 1e+06      | vanilladot | 62.5382    |\n",
       "| 1e+07      | vanilladot | 54.5872    |\n",
       "| 1e+08      | vanilladot | 66.3609    |\n",
       "| 1e-09      | rbfdot     | 54.7401    |\n",
       "| 1e-08      | rbfdot     | 54.7401    |\n",
       "| 1e-07      | rbfdot     | 54.7401    |\n",
       "| 1e-06      | rbfdot     | 54.7401    |\n",
       "| 1e-05      | rbfdot     | 54.7401    |\n",
       "| 1e-04      | rbfdot     | 54.7401    |\n",
       "| 1e-03      | rbfdot     | 54.7401    |\n",
       "| 1e-02      | rbfdot     | 56.1162    |\n",
       "| 1e-01      | rbfdot     | 85.9327    |\n",
       "| 1e+00      | rbfdot     | 87.1560    |\n",
       "| 1e+01      | rbfdot     | 91.4373    |\n",
       "| 1e+02      | rbfdot     | 95.7187    |\n",
       "| ... | ... | ... |\n",
       "| 1e-03     | anovadot  | 58.8685   |\n",
       "| 1e-02     | anovadot  | 86.2385   |\n",
       "| 1e-01     | anovadot  | 86.2385   |\n",
       "| 1e+00     | anovadot  | 86.3914   |\n",
       "| 1e+01     | anovadot  | 87.3089   |\n",
       "| 1e+02     | anovadot  | 90.6728   |\n",
       "| 1e+03     | anovadot  | 90.6728   |\n",
       "| 1e+04     | anovadot  | 90.8257   |\n",
       "| 1e+05     | anovadot  | 88.5321   |\n",
       "| 1e+06     | anovadot  | 81.9572   |\n",
       "| 1e+07     | anovadot  | 68.9602   |\n",
       "| 1e+08     | anovadot  | 77.3700   |\n",
       "| 1e-09     | splinedot | 54.7401   |\n",
       "| 1e-08     | splinedot | 54.8930   |\n",
       "| 1e-07     | splinedot | 54.8930   |\n",
       "| 1e-06     | splinedot | 55.8104   |\n",
       "| 1e-05     | splinedot | 57.7982   |\n",
       "| 1e-04     | splinedot | 62.3853   |\n",
       "| 1e-03     | splinedot | 78.2875   |\n",
       "| 1e-02     | splinedot | 81.0398   |\n",
       "| 1e-01     | splinedot | 94.4954   |\n",
       "| 1e+00     | splinedot | 96.6361   |\n",
       "| 1e+01     | splinedot | 97.8593   |\n",
       "| 1e+02     | splinedot | 97.8593   |\n",
       "| 1e+03     | splinedot | 97.8593   |\n",
       "| 1e+04     | splinedot | 97.8593   |\n",
       "| 1e+05     | splinedot | 97.8593   |\n",
       "| 1e+06     | splinedot | 94.3425   |\n",
       "| 1e+07     | splinedot | 87.7676   |\n",
       "| 1e+08     | splinedot | 86.5443   |\n",
       "\n"
      ],
      "text/plain": [
       "    C_Value Kernal_Name Accuracy\n",
       "1   1e-09   vanilladot  54.7401 \n",
       "2   1e-08   vanilladot  54.7401 \n",
       "3   1e-07   vanilladot  54.7401 \n",
       "4   1e-06   vanilladot  54.7401 \n",
       "5   1e-05   vanilladot  54.7401 \n",
       "6   1e-04   vanilladot  54.7401 \n",
       "7   1e-03   vanilladot  83.7920 \n",
       "8   1e-02   vanilladot  86.3914 \n",
       "9   1e-01   vanilladot  86.3914 \n",
       "10  1e+00   vanilladot  86.3914 \n",
       "11  1e+01   vanilladot  86.3914 \n",
       "12  1e+02   vanilladot  86.3914 \n",
       "13  1e+03   vanilladot  86.2385 \n",
       "14  1e+04   vanilladot  86.2385 \n",
       "15  1e+05   vanilladot  86.3914 \n",
       "16  1e+06   vanilladot  62.5382 \n",
       "17  1e+07   vanilladot  54.5872 \n",
       "18  1e+08   vanilladot  66.3609 \n",
       "19  1e-09   rbfdot      54.7401 \n",
       "20  1e-08   rbfdot      54.7401 \n",
       "21  1e-07   rbfdot      54.7401 \n",
       "22  1e-06   rbfdot      54.7401 \n",
       "23  1e-05   rbfdot      54.7401 \n",
       "24  1e-04   rbfdot      54.7401 \n",
       "25  1e-03   rbfdot      54.7401 \n",
       "26  1e-02   rbfdot      56.1162 \n",
       "27  1e-01   rbfdot      85.9327 \n",
       "28  1e+00   rbfdot      87.1560 \n",
       "29  1e+01   rbfdot      91.4373 \n",
       "30  1e+02   rbfdot      95.7187 \n",
       "... ...     ...         ...     \n",
       "115 1e-03   anovadot    58.8685 \n",
       "116 1e-02   anovadot    86.2385 \n",
       "117 1e-01   anovadot    86.2385 \n",
       "118 1e+00   anovadot    86.3914 \n",
       "119 1e+01   anovadot    87.3089 \n",
       "120 1e+02   anovadot    90.6728 \n",
       "121 1e+03   anovadot    90.6728 \n",
       "122 1e+04   anovadot    90.8257 \n",
       "123 1e+05   anovadot    88.5321 \n",
       "124 1e+06   anovadot    81.9572 \n",
       "125 1e+07   anovadot    68.9602 \n",
       "126 1e+08   anovadot    77.3700 \n",
       "127 1e-09   splinedot   54.7401 \n",
       "128 1e-08   splinedot   54.8930 \n",
       "129 1e-07   splinedot   54.8930 \n",
       "130 1e-06   splinedot   55.8104 \n",
       "131 1e-05   splinedot   57.7982 \n",
       "132 1e-04   splinedot   62.3853 \n",
       "133 1e-03   splinedot   78.2875 \n",
       "134 1e-02   splinedot   81.0398 \n",
       "135 1e-01   splinedot   94.4954 \n",
       "136 1e+00   splinedot   96.6361 \n",
       "137 1e+01   splinedot   97.8593 \n",
       "138 1e+02   splinedot   97.8593 \n",
       "139 1e+03   splinedot   97.8593 \n",
       "140 1e+04   splinedot   97.8593 \n",
       "141 1e+05   splinedot   97.8593 \n",
       "142 1e+06   splinedot   94.3425 \n",
       "143 1e+07   splinedot   87.7676 \n",
       "144 1e+08   splinedot   86.5443 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ae345d-4e41-4c91-aa25-50fa104f9482",
   "metadata": {},
   "source": [
    "### KNN Classification\n",
    "Next I am going to fit our data with a KNN model and test to see how well it predicts credit approval. I have to determine a good value of k which defines how many neighbors will be compared to determine the classification of the point. The equation I am going to train it on is:\n",
    "\n",
    "\\\\[R1 ~= f(A1, A2, A3, A8, A9, A10, A12, A14, A15)\\\\]\n",
    "\n",
    "I am going to use Leave-One-Out Cross-Validation (LOOCV). This is done by leaving out a single data point dataset and then training the model with the remaining data points. The removed data point is then used as a test case to evaluate the model's performance. For each of the k values we want to test we will loop through each row of data, using that row to test our model and using all of the rest of the data to use as neighbors. I am going to use loops to accomplish this but I found that using the train.kknn() function is another option as it also uses the LOOCV method. For the purpose of this homework to get practice using loops in R I am not using the train.kknn() option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4e0e225-bf77-496f-b7aa-dbee82b59771",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_len = 60\n",
    "knn_results_df <- data.frame('k_value' = numeric(), 'Accuracy' = numeric(), stringsAsFactors = FALSE)\n",
    "\n",
    "for (k_val in 1:k_len) {\n",
    "    knn_prediction <- array(0, nrow(data))\n",
    "    for (i in 1:nrow(data)) {\n",
    "        train_data <- data[-i,]\n",
    "    \ttest_data <- data[i,]\n",
    "    \tknn_model = kknn(as.factor(R1)~., train_data, test_data, k=k_val, scale = TRUE)\n",
    "        knn_prediction[i] <- predict(knn_model)\n",
    "    }\n",
    "    # Subtracting 1 from knn_prediction b/c it gives 1 and 2 instead of 0 and 1 as result\n",
    "    knn_accuracy <- 100 * sum(data[,11] == knn_prediction-1)/nrow(data)\n",
    "    knn_results_df <- rbind(knn_results_df, data.frame('k_value' = k_val, 'Accuracy' = knn_accuracy))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d09e054-be7e-41a1-a65c-ae02555500ff",
   "metadata": {},
   "source": [
    "Now that I have the results of our various knn models with k values of 1-60 in our knn_results_df, I will use that to determine the answer to question 2.2 - Part 3 below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac9a54a-4cc9-4388-bc37-d6bdb18f62e0",
   "metadata": {},
   "source": [
    "### Answer to Question 2 - Part 1\n",
    "Part 1 asked to find a good classifier for the data using the SVM function ksvm and the kernal VannillaDot. As can be seen from the results_df shown above and the graph below, our model achieved 86.39% accuracy in predicting credit card approval for the data set with multiple C values (.01, 1, 10, 100, 100000). I will choose **0.01 as our C value for the answer to this question.** The reason for this selection, equation for the choosen classifier, and the results of it's predictions can be found below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3168d2e-2d8a-4dcf-a06c-e9b667c4f676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA8AAAAFoCAMAAACv2GIDAAAANlBMVEUAAABNTU1oaGh8fHyM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\nAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the values from the results_df for just the vanilladot kernal)\n",
    "desired_kernels <- c(\"vanilladot\")\n",
    "\n",
    "options(repr.plot.width = 8, repr.plot.height = 3)\n",
    "\n",
    "ggplot(data = subset(results_df, Kernal_Name %in% desired_kernels), aes(x = factor(C_Value), y = Accuracy, color = Kernal_Name, group = Kernal_Name)) +\n",
    "  geom_point() + \n",
    "  geom_line() +\n",
    "  labs(x = \"C-Value\", y = \"Accuracy\", title = \"SVM Model Prediction Accuracy - VanillaDot\") +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d456a885-ff4d-47da-ad81-45633900d039",
   "metadata": {},
   "source": [
    "I chose **0.01 as my C-value** for the classifier, which matched an **accuracy of 86.3914%** with serveral other C-values. I choose to use the smaller C value because it puts a larger emphasis on increasing the margin. Depending on the cost value of a mistake for approving a credit card this may or may not be appropriate, but based on how easy it is to be approved for a credit card my assumption is that issuers side more liberally on approval. Rerunning the model I get the predicted values vs. the actual values and the coefficients for our models equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da975d9e-bbd5-46a9-b630-27c03c1e03f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Setting default kernel parameters  \n",
      "Example of output from model, full results inspected but not shown to keep response to managable length:"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Predicted_Values</th><th scope=col>Actual_Values</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "\t<tr><td>1</td><td>1</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " Predicted\\_Values & Actual\\_Values\\\\\n",
       "\\hline\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\t 1 & 1\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| Predicted_Values | Actual_Values |\n",
       "|---|---|\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "| 1 | 1 |\n",
       "\n"
      ],
      "text/plain": [
       "  Predicted_Values Actual_Values\n",
       "1 1                1            \n",
       "2 1                1            \n",
       "3 1                1            \n",
       "4 1                1            \n",
       "5 1                1            \n",
       "6 1                1            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rerun the model and save the results and coefficients of the model\n",
    "vsm_model <- ksvm(as.matrix(data[,1:10]),as.factor(data[,11]),type=\"C-svc\",kernel=\"vanilladot\",C=0.01,scaled=TRUE)\n",
    "a <- colSums(vsm_model@xmatrix[[1]] * vsm_model@coef[[1]])\n",
    "a0 <- -vsm_model@b\n",
    "pred <- predict(vsm_model,data[,1:10])\n",
    "accuracy <- round((sum(pred == data[,11]) / nrow(data))*100,4)\n",
    "\n",
    "# Compare predicted values to actual values to get a sense of how our model looks\n",
    "prediction_df <- data.frame('Predicted_Values' = pred, 'Actual_Values' = data[,11], stringsAsFactors = FALSE)\n",
    "cat('Example of output from model, full results inspected but not shown to keep response to managable length:')\n",
    "head(prediction_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3231af-d6e7-49af-8891-1fa9d1ed597a",
   "metadata": {},
   "source": [
    "The equation for the model can be seen below, the coefficients were rounded to show the entire equation. The coeficients full values can be found below in the output of the next cell.\n",
    "\n",
    "\\\\[(-0.0002X_{i1} - 0.0015X_{i2} + 0.0014X_{i3} + 0.0073X_{i8} + 0.9916X_{i9} - 0.0045X_{i10} + 0.0071X_{i11} - 0.0005X_{i12} - 0.0017X_{i14} +  0.1055X_{i15} + 0.0820) Y_{i} \\ge 1\\\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f43d048f-0c25-4c3c-b69a-c48da88bae8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a C-Value of 0.01 our model correctly predicted  86.3914 % of the outcomes for the datatset. The coefficients for the model are shown below: \n",
      "a = -0.0001500738 -0.001481829 0.001408313 0.007286389 0.991647 -0.004466124 0.00714829 -0.0005468386 -0.001693058 0.1054824 \n",
      "a0 = 0.08198854 \n"
     ]
    }
   ],
   "source": [
    "cat(\"With a C-Value of 0.01 our model correctly predicted \", accuracy, \"% of the outcomes for the datatset. The coefficients for the model are shown below:\", \"\\n\")\n",
    "cat(\"a =\", a, \"\\n\")\n",
    "cat(\"a0 =\", a0, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c590078-c0ba-4567-acc3-ae1830a3e6b2",
   "metadata": {},
   "source": [
    "### Answer to Question 2.2 - Part 2\n",
    "For Part 2 I looked at the 6 different kernals listed in the R documentation on the ksvm function (https://www.rdocumentation.org/packages/kernlab/versions/0.9-32/topics/ksvm). Looking at the results of the various kernal types in the dataframe shown above and the graph below, two models reach 100% accuracy (rbfdot, laplacedot). **The laplacedot kernal saw a 100% prediction over a wider range of C values** (100 - 100000000) than the rbfdot which got all of it's prediction correct only at the largest two C values. This gives laplacedot the slight edge but both proved to correctly predict 100% of the credit card approvals at the right C value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c413417-153d-4c40-8f67-58a498c761d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA8AAAAFoCAMAAACv2GIDAAAAS1BMVEUAAAAAqf8AvmcAv8RN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\nAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "options(repr.plot.width = 8, repr.plot.height = 3)\n",
    "ggplot(data = results_df, aes(x = factor(C_Value), y = Accuracy, color = Kernal_Name, group = Kernal_Name)) +\n",
    "  geom_point() + \n",
    "  geom_line() +\n",
    "  labs(x = \"C-Value\", y = \"Accuracy\", title = \"Model Prediction Accuracy - Various Kernals\") +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8095750-d79b-4c03-86ad-8460fc1388d8",
   "metadata": {},
   "source": [
    "### Answer to Question 2.2 - Part 3\n",
    "For part 3 I am using the k-nearest-neighbors classification function kknn contained in the R kknn package to come up with a good value of k, and then showing how well it classifies data points in the full data set. I ran the model for a range of 1:40 k-values and found that the most accurate model used a **k-value of 12** which **correctly predicted 85.21%** of the results. A graph of the k-values vs. their accuracy can be seen below as well as the maximum accuracy value and corresponding k-value from our knn_results_df dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfcc80bf-550d-4504-a4c2-b5e4921f754d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Accuracy value: 85.3211 \n",
      "Corresponding k-value: 12 \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA8AAAAFoCAMAAACv2GIDAAAANlBMVEUAAABNTU1oaGh8fHyM\njIyampqnp6eysrK9vb3Hx8fQ0NDZ2dnh4eHp6enr6+vw8PD/AAD///9JBqxuAAAACXBIWXMA\nABJ0AAASdAHeZh94AAAYVElEQVR4nO2diZaiMBQFg2u3y4j//7PDpiY8NiEJeXTVOTOKpuF2\nQnVCQDFPAFCLWTsAAMwHgQEUg8AAikFgAMUgMIBiEBhAMQgMoBgEBlAMAgMoBoEBFDNf4Ozw\nc2+e3n8OWe8GTO+yadj/jGyq/Jn2eh7HrrX3hjW9AQEUM1/gwrxT8/Rk+kWaIHCh8Nim+tYz\nUeBLsY3LpJIAqlgi8O7Vq2W7mQLXj9fMDPfBXWuf2vdWHMzJHL4oD6CEJQKfzbV6di2eLRG4\nWMFwF7xU4IfZPXfmMf0HAJSwROB7M4Y+mVuj0+1YHGweb3WBy95k50a0674YJ1/qH7PWYD8z\n5rGruslPWXsdTeHiBXN8NMPv96vWdotXTpnZOQPmc9HD/5jz8+mswn7arMfJcSrGFfur8zPl\nX4KK9xOAVVki8LMZQ2dZI8ClOaSt7DlVT4/VO7/166fnoMCHqoRV1l5HXbh+IWsJbG+3kO6T\noSEzpXvNgP+9CvupI3CdIxO/S1Y+qdf7+/5zALAmiwQ+VWPoa7G7VwIU/fC5MKXY2+/Vwm+h\nVla+c6t6stu+3Pu7BL6UQ+jCvMfTLWuvo9lCdnk+9qVf1iSWs93Cs+vzcbAH5ZeqQz3W7lmr\nEGtrBK5ynCtFz9V6PgXvTc+7N/cnwPosEvha9ZOlxpUAp2ZIfSwfT/XE1CMz5fNqxPooReoQ\nuDC0Mrsartpl7XVUhY/VC9X41RLY2W7dZz7sQ+RD9VKtsb0KsbZG4CpHc8zc3uyhWT0jaEiC\nRQKXg+dyrNns57umW6q6qdek0cGUz1/0nEayxtZ2WXsdzRYe9ua7tmup2PAaPGfVD1urEGtr\nna+6Xc779mZv1Z8BRtCQCMsELjvfqzk2e/1717c92Bvb1C6Bs4M9u9VVdm/aW3AE7tquVfL8\nXuO5axX2U3ujP9krhjPjfTA3RtCQDMsEvhR95+l9ZNspcPsijG6Bnl3myXXMEjh7C5x1raK9\n8ebpT3EsfPq9C4HLLpgRNKTCMoGfhRPZexfvGspWB6OZubk/Jp5+ljrKPt5eZTOG0Jf3JRzV\n8au1CrE2W+BdfSjc3mzV+zKChlRYKPDRVDPIr8mk6vrkejLpUIv4U75zrN+41ZPN7hraS3ZZ\nex2vzT2bg1pnEsvarhD40FxuUk6XH5xVOGsrFb2ILvzS3mw16mAEDamwUODyDOzva6EYb57q\n0zm30rrstz6pW53nKc/aZH2nkewlu6y9jmZz2e194ufeud22wPZwt5yMslZhPd2bw8M+X1WW\n/XmdwbILlu9kjKAhFRYKXIxuzedKJudCjvp6ivPb86ELOewlq6y9jvrt+oqKss/f9V7I4a77\nbF3ScSmHvp9VWE+v9TbdY+Caq1Ow3hojaEiEhQIXiu0/C+6llL/WpZT38vLGH6tk66m99Clr\nr+Pl1c5kldu33Wcg3bqU0llbZn+OsFp4r8J+et21NlTOQmfHa3MEbf1M+UeLETQkwnyB/yw/\njKAhGRD4W4rD8+t4KYAoIPB3vA/PAVIAgb9j9z48B0gABAZQDAIDKAaBARSDwACKQWAAxSAw\ngGIQGEAxAQT+53+VMyGJhCQSzUnmCvwPANZiucBDbvtf5UxIIiGJRHMSBI4CSSQkkSCwA0kk\nJJFoToLAUSCJhCQSBHYgiYQkEs1JEDgKJJGQRILADiSRkESiOckfFTjP84nrmF5yXpLYkESi\nOcnfFDjPp3o5veS8JNEhiURzkj8pcJ5P9XJ6yXlJ4kMSieYkCDwIAoeDJBIEdkBgCUkkmpMg\n8CAIHA6SSBDYoSdJ3swtj3s5veS8JCtAEonmJH9PYKs/Hela7ZIhkqwCSSSak/w5gfOBpYGS\nC4fRadfJOpBEgsAOMonwsFdMWdJvkrUgiURzkkQF9n/9U++xbN7eWl/J7oI9SwNJ1oUkEs1J\n0hTY//VPA7PJrbf6S3YV7FkaSLIyJJFoTpKkwP5P3gyt0X1voORQwZHImneRUJBEgsA9SSYI\n7DK54PiPuUnWhiQSzUkQmB54JUgi2YjA/o+BB/Vy3xsoOVQQgb+FJJKtCOx7FlpMIA9tbaDk\nUMF86JItzbtIKEgi2YzASy+cqHkl8bGuifTk1ryLhIIkkg0J7MO6OomXvwXT6dya5l0kFCSR\nILDDv3pNken6g6F5FwkFSSQhBM4KXo9ZmBAd+BB48Kg0JCPXbK2K5p01FJqTjAqcvf6bJO+8\nEB14UM/TuSgPm14viETzzhoKzUkSFTh/LhbY18nk5dteM4lA884aCs1JJgs82V8E/uLirvho\n3llDoTnJdIHdQ+DA9z/N3//NX0WNlzjLtr1mEtgkXwj8msR6D6VH8dUDe+mCPWRZvm164C5I\nIgl5DGw/eg4h8SWwhyizN95eSkNhzTtrKDQnSVngZQbn6TVMCganVicpoDnJd5NYigTOPSXx\nQ5MkAYOTq5ME0JzkK4GnzURHFbj/k0OekvjhfVX2yMcq4iVZH5JIQl+JFSiEJHceBsr1Hejm\nvpL4YeIHG6MmWRuSSDZzLfREgXvPsObekvhh2lcLxE2yNiSRILDzcyk2DAK/IIlkawKPGYzA\nC5KsDUkkCOz8VJINg8ANJJH8MYFfU7q5eNlfEj/YSboir5NkXUgi2ZzAQzt6313KPs9Tbpi1\nOuGU62QtNCfRK3Det6RD4LU64bTrZB00J0lT4LzjWatE+428Y9SdeMOs0gknXieroDlJ6gJL\ng3sPIXPx+YXkG2aF67KSr5MV0JxEncADk7jirfQbJv6UdPp1Eh/NSbQJPHQaVbyXfsPEPyuc\nfp3ER3OS5AVuT1YhcJgk8SGJBIEReGaS+JBE8lcE7vsxdcfA8b/3R0GdREdzkvQFbts8tMO3\n3tPQMAi8PpqTqBL4211dS8MwhF4XzUk0Cfz1fq6mYSIarKZOIqI5iQKBX9+v8/1erqZhEHhV\nNCdRIfDcj+/oaZjeafXepVBJ4kESySYFnn+mRVHDdF4b6vzenuarFdVJNDQnSV/gBadKNTVM\nx9eKOL+4rzPGmuokFpqTIHAUFgjsEiFJLEgiQWDfSTwxIUnXx67ogaOgOUn6Ai84+NPVMCOf\nu0LgYGhOMlfgoDdPbN2JM/8jt+Z0fsvyd3Z/8bx+KXIoSJHlAg+5vXgNvk6LKvvLan+IcupX\nCYVJEgWSSDYyhP6jAk+85mxZ7WirkxhoToLAUZiYpD7KHTvSXXSHNHV1EgHNSRA4Cl8IPMXM\nBRNa6uokApqTIHAUJg6hvxN4+6fWIqE5CQJHAYElJJEgsIO2hkHgddCcBIGj8M0xsNeCc5NE\ngCQSBHbQ1zCTtURgj2hOgsBRCJKEIbQnNCdB4CggsIQkEgR20NwwU9j2t5TEQ3MSBI5CoCQz\nKmrzdTIDzUkQOAqhkmz4i/4iojkJAkcBgSUkkSCwg+aGmch2vys7IpqTpCiwt29J1twwU/m2\nsv5CnXyL5iQIHAUElpBEgsAOmhtmMhu9X1RMNCdB4CiETPJddf2NOvkOzUkQOAoILCGJBIEd\nNDfMF3xVX3+kTr5CcxIEjkLYJF98NMnL/ZX84L1O/uT3hCFwFMILPK3SPN0hzQu+6+RvfkYa\ngaMQdgg9+fs5fN2fxQue6+SPfksJAkcBgSUILEFgB80N8wUIXILAPWQFr6eBQrRA4C/57qu0\nEvE3jMApJFlAAIGz93/PDIHnEjjJ1A54wXdp+cdvnSz53TTvJ18InNEDzyZCktFaq/fufx7r\ndxle6yRvPX6F5v1kusAZQ+j5xEgydkel+mGTAtu3UV43yULiCRzy5qfcAncWg9Xm3mk4cJLY\nbPl366ZX4N35KgyuJ7Fe/8ZZ+vfMXweh+S/rDLorrr7hoZskiT7YR520frnqpf6CPUtOkqGC\nk5em/1iLxT2wMSY7XuxX7MNfBJ5LnCRdNdeenN2UwJ0zz7210LPkJBkqOHlp+o+1WSzw4/dQ\nOGz2v/fXKy+Ba6asEoElkZLIqhNnR//1FIyPh69e6j51NFYLPXUyVnDy0vQfE3g5Br6cssLh\nXdMP210vPfBcUhM4BYPjC+zSlaSzYAD6fiM/k1j3k6m64WoBgX0QK4ncmft21vUNDibwWC2M\nC7xsafqPCXwIfDtU3e91bw7VcvQrsRB4AeKAsL2/bEng3gPKkVoYE7hn/ZOXRgsGFPiyf4+e\nzczrpBFYEi+JU3t53j/jurrBngTufsMt1TkT/Hnl33DB75dGC/YpvFjgnTGH2+utaR2uhxAu\nCLwEZzcaTLK2wR6G0FPeGZjzfb2zxrxA98aWn0Y63brLfQECSyImycUThw0JPPQLjNTC683c\nShK5Pkb+vE6lfRppXpiFIVwQeBn51EHa0MFYBMJecjtcC1axkUFtQMZG7JNoH+ceqhfM7t5R\ndiIILIkt8KRpksFy4Qm7nwzXgii4Tj0MzplNoyXwqZ65MuY4PxUCS2IOoacKvOae6yaZxRR/\ndQg88RRTNy2BM1NdC32bOwNdgsASBB5KMoeR3KoEdvluFeJaaPdxDggsQeChJHPYksBLkrRE\nPZjj4/l8nJrLsGaBwJLox8BTkmgWeDT29N9tzVrwfgx8z6qrKE224GwSAkuiJhncC8RH51Yz\neEmdTAg93YU1/4q5256RpD1Ufpx2xuxOCyahEbiDlJOstfsuqBPPiVNunTHS+1pZj42juWFC\n0ZVkHYMRWILADpobJhSdSVbphOfXie+0ibfOIG2B608SmhVnoRE4KD1J5Kce1koyQogZp+Rb\nZwB5IQcCByD9JPGnpGfVSZCU6bdOP+JCjtve3B97I77bLmQIBwQOyrDAMQ2eUydhUqbfOv3I\nCznO5vJ8rHgeGIGDgsB+koTBh8AX87PqlVgIHBQE9pMkDIsFPpjfu9k9rwjsFwVJFAmcQpIw\nLBa4NHdfzmGt92kkBA5Kf5LY12XNqJNAc+UaWqcP8Z1Yu+fzaMwpaggHBA7KWJJ4nfDXdRIs\nmp7WkXAhRxQ0JYll8Ld1Ei6XptZp0xJ4v2DoPD+EAwIHZUKSSJ3wd3USMpSq1mkhzgOvEcIB\ngYMyKUmU67JGjsbdpaBhlLWOQ0vY237RB5FmhnBA4KBMSxJjSnp4PjxmFG2tYyPvTjjtUspw\ntz79Gzd4TZzmfGsC214zSbosF3jI7fk/WkIPHJRveuB1+r28i1WSxIdZaAfNDRMKNQInkCQ+\nGxDYZ1NpbphQTEyypjYdx8BBg+hrHYvkhtAIHJapSfLgtysYFLidJCj6WucDAkdBbZIVzr7G\n/3YQta3z7BlC3/fnqCFsEDgsX1//FMyn/u8GiY3e1uk7Bn6YBQYjsERzko6bDIdMgsBf0TNU\nZgjtF91JYn6IT9fX6/nGl8C/c2/uPS+EDQKHJfGP0a/yDbeaW6dvEmvB5wkRWKI7STyB1X1D\ntWd8CZyt93lgBA4LAks0tw4XckRBeZJYAuu7yYtnENhBc8OEYl6SEB/pk0nWuseY5tZpC3yo\nXjC7BR8qRGDJNpL47YRFEpX3SfTLYoFP9fmjFb/UDoHDknDrILCPb+Sobslw4zywXzaTxGMn\n3E6i9FbjXvEwC+0+xglhg8BhWZykfV3W0CcPJi9F/krqFppbR3yx+/FR3uV7vVurIHBYlidx\np6SHPvs3eSnCRwaH0Nw6LYHvWXMi+BYzhEXYaZLV2FIS56zw0KfvJy/F+ND+EJpbpz1Ufpx2\nxuwWfbMdAku2lCQPhYffbh6aWye188AIHBh6YInm1kHgKGwqiWObq97MpXX9Vd06qV3IgcCB\n8ZEkb00g97311Sy0h1xz0dw6qV3IgcCBIYlEc5LULuRA4MCQRKI5SWoXciBwYEgi0ZwktQs5\nEDgwJJFoTjJ+IUdWYD8GCGGBwIEhiURzktELObLmv+y94D+EBQIHhiQSzUlGj3UR2AckkZBE\nElDgp/3oOYQFAgeGJBLNSboFvp7eqvYIHOrGp9wIFmCUQYEvx8xY3wv9nrxiEms+JJGQRLK8\nB74cy0no4+X9AkNoH5BEQhLJQoFre415WK8hsA9IIiGJZJnATd/rXoUVdxba7yXtmhsmFCSR\naE7iCnx4PNuXUSKwD0giIYkkQA8c90osBA4NSSSak4weA8cI8QGBQ0MSieYkPbPQ16ghPiBw\naEgi0Zxk/DxwhBAfEDg0JJFoTjJ6JVaMEB8QODQkkWhOktiX2iFwaEgi0ZwEgaNAEglJJAjs\noLlhQkESieYkCBwFkkhIIkFgB80NEwqSSDQnQeAokERCEgkCO2humFCQRKI5CQJHgSQSkkgQ\n2EFzw4SCJBLNSRA4CiSRkESiXWDPt6jT3DChIIlEcxIEjgJJJCSRILCD5oYJBUkkmpMgcBRI\nIiGJBIEdNDdMKEgi0ZwEgaNAEglJJAjsoLlhQkESieYkCBwFkkhIIkFgB80NEwqSSDQnQeAo\nkERCEkk8gcPcNZGbiwJMYLnAQ27P/1F64OCQRKI5CQJHgSQSkkgQ2EFzw4SCJBLNSRA4CiSR\nkESCwA6aGyYUJJFoTpKUwJ79Vd0woSCJRHMSBI4CSSQkkSCwg+aGCQVJJJqTIHAUSCIhiQSB\nHTQ3TChIItGcBIGjQBIJSSQI7KC5YUJBEonmJAgcBZJISCJBYAfNDRMKkkg0J0HgKJBEQhIJ\nAjtobphQkESiOQkCR4EkEpJIENhBc8OEgiQSzUkQOAokkZBEgsAOmhsmFCSRaE6CwFEgiYQk\nEgR20NwwoSCJRHOSlAT27a/qhgkFSSSakyBwFEgiIYkEgR00N0woSCLRnASBo0ASCUkkCOyg\nuWFCQRKJ5iTjAmcF9mOAEA0IHAGSSDQnGRU4a/7L3gv+QzQgcARIItGcBIGjQBIJSSQBBX7a\nj55DNCBwBEgi0ZxkrsAhbnrK3YEBJvGFwPbkFZNYcyGJhCSSwD0wAs+FJBKSSMIKPM1fBO6A\nJBKSSIIKPNFfBO6AJBKSSEIKPNVfBO6AJBKSSAJeiZVlUy/FQmAJSSQkkXAttIPmhgkFSSSa\nkyQksHd/VTdMKEgi0ZwEgaNAEglJJAjsoLlhQkESieYkCBwFkkhIIkFgB80NEwqSSDQnQeAo\nkERCEgkCO2humFCQRKI5CQJHgSQSkkgQ2EFzw4SCJBLNSRA4CiSRkESCwA6aGyYUJJFoToLA\nUSCJhCQSBHbQ3DChIIlEcxIEjgJJJCSRILCD5oYJBUkkmpMgcBRIIiGJRLPA/v1V3TChIIlE\ncxIEjgJJJCSRILCD5oYJBUkkmpMgcBRIIiGJBIEdNDdMKEgi0ZwksMB5ns9a8p9kXUgiIYkk\nNYHz3PZy+pL/JCtDEglJJIkJnOe2l9OX/CdZG5JISCKJJ/Cku5jm3xL4rqoA22C5wENuv5/R\nA78giYQkksSG0BwDvyCJhCSS1ARmFrqBJBKSSJITeF1IIiGJRHMSBI4CSSQkkSCwA0kkJJFo\nToLAUSCJhCQSBHYgiYQkEs1JEDgKJJGQRILADiSRkESiOQkCR4EkEpJIkhAYAGKBwACKQWAA\nxSAwgGIQGEAxCAygGAQGUAwCAygGgQEUg8AAivEucFbge52aedUH9eJAnbSYu5/4Fjh7/wcl\nr/qgXhyqnZQ6+TB7P0HgsCBwF9kTgV0QOGUQ2KWpD+rkQ2Y/InBiILALAguyZzbvsAKBw8PO\n6pI9qZM2L3sROEHYWR3eVUGdfOAYOF0y978/T1ZDndggcLJkn/+plzf0wC4InCqZ9UC9vEFg\nl2QE5uoal9dwkXpx4UqsFqlciQUAEUFgAMUgMIBiEBhAMQgMoBgEBlAMAgMoBoEBFIPAAIpB\n4I1i6pbdG3OpnjzMrnlnZ+6iGGiF9tsotZlvf8untbd3sxfFQC2030apzNyb7N3b/ppz9Xg2\nv+1ioBfab6OUZtr+vsfQO/N4Pq8HY7JTU6yWuPr/cTTm+FgjL8wDgTdK4ePeGSw/D9UYuhpB\nX0zFSQiclS/vutYHaYLAG6XoYY0zWC6sLcfQ1Qh6V/53e8n7EfhcOn0yP6skhjkg8EYxlcF3\n56Wya61G0EVHfDnvpcC7+tkhelqYCwJvlHKAvK9Hw9VwuXg8Fj7fazv3pnnRFdiY1+ugBNpq\no5QW3jNzfH4ELsfQ5+q00tHsfi53BN4AtNVGqSy8uofBRYe8s2acHYHvnyE0KIIW2yh1N/rj\nHAYfzaXqkos3r8/H+xg4Kyyvl07lJNavO3kNSYPAG6UZBx9tG8uzR9WFWSdjHwNXS+fy2aM6\njWRuqySGOSDwRnkdyO7LTvVF9nr1aMz++h4+n7Li6LgeSFdvxM4K80FgAMUgMIBiEBhAMQgM\noBgEBlAMAgMoBoEBFIPAAIpBYADFIDCAYhAYQDEIDKCY/+QelDFwXZWFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a graph of k-values vs. accuracy\n",
    "options(repr.plot.width = 8, repr.plot.height = 3)\n",
    "ggplot(data = knn_results_df, aes(x = k_value, y = Accuracy)) +\n",
    "  geom_point(color = \"red\") + \n",
    "  geom_line(color = \"red\") +\n",
    "  labs(x = \"K-Value\", y = \"Accuracy\", title = \"Model Prediction Accuracy\") +\n",
    "  theme_minimal()\n",
    "\n",
    "# Find the row index with the maximum Accuracy and the corresponding K-Value\n",
    "max_index <- which.max(knn_results_df$Accuracy)\n",
    "max_k_value <- knn_results_df$k_value[max_index]\n",
    "max_accuracy <- knn_results_df$Accuracy[max_index]\n",
    "\n",
    "# Print the results\n",
    "cat(\"Maximum Accuracy value:\", max_accuracy, \"\\n\")\n",
    "cat(\"Corresponding k-value:\", max_k_value, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
